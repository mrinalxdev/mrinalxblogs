<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <title>Flash Attention Part I</title>
        <script src="https://unpkg.com/@tailwindcss/browser@4"></script>
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
        <script
          id="MathJax-script"
          async
          src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
        ></script>
        <script
          async
          src="https://www.googletagmanager.com/gtag/js?id=G-4VP1VKLPKJ"
        ></script>
        <script>
          window.dataLayer = window.dataLayer || [];
          function gtag() {
            dataLayer.push(arguments);
          }
          gtag("js", new Date());
    
          gtag("config", "G-4VP1VKLPKJ");
        </script>
      </head>
<body class="bg-orange-100 w-full max-w-[95%] md:w-[55%] mx-auto">
    <nav class="mt-6">
        <div>
          <h1 class="font-serif text-4xl">Mrinal's Blog</h1>
          <div class="mt-5 flex flex-wrap gap-5">
            <a
              href="../index.html"
              class="relative after:content-[''] after:absolute after:w-0 after:h-[2px] after:bg-black after:bottom-[-4px] after:left-0 hover:after:w-full after:transition-all after:duration-300"
              >Home</a
            >
            <a
              href="./blog.html"
              class="relative after:content-[''] after:absolute after:w-0 after:h-[2px] after:bg-black after:bottom-[-4px] after:left-0 hover:after:w-full after:transition-all after:duration-300"
              >Blogs</a
            >
            <a
              href="../research-notes/research.html"
              class="relative after:content-[''] after:absolute after:w-0 after:h-[2px] after:bg-black after:bottom-[-4px] after:left-0 hover:after:w-full after:transition-all after:duration-300"
              >Research Notes</a
            >
            
          </div>
        </div>
      </nav>
  
      <hr class="mt-3" />

      <div class="my-5 border rounded-xl p-4">
        <h1 class="text-xl font-serif mb-6">Pre-Requisites to know <span class="text-sm text-gray-500">[Click on the topic header of these below to make it collapse or expand]</span></h1>
  
        <div class="ml-4">
          <ul class="list-disc ml-4">
            <li class="cursor-pointer font-serif " onclick="toggleCollapse('item1')">
              Attention : <a href="../blogs/attention.html">[Link]</a>
              <ul id="item1" class=" pl-4 mt-1 space-y-1 transition-all duration-300 hidden ease-in-out">
                <li>Complete the attention blog to get an idea why even Multi-Head is needed. Its like a homework you can do to understand better about the topic of this blog. </li>
              </ul>
             </li>
            
             <li class="cursor-pointer font-serif " onclick="toggleCollapse('item2')">
                Articles you should go through once : <a href="https://www.hopsworks.ai/dictionary/flash-attention">[Link]</a>
                <ul id="item2" class=" pl-4 mt-1 space-y-1  transition-all hidden duration-300 ease-in-out">
                  <li>Go through these articles after or before reading this blog, its up to you anon !! </li>
                </ul>
               </li>
               <li class="cursor-pointer font-serif " onclick="toggleCollapse('item3')">
                Tile Matrix Multiplication : <a href="https://alvinwan.com/how-to-tile-matrix-multiplication/">[Link]</a>
                <ul id="item3" class=" pl-4 mt-1 space-y-1 hidden transition-all duration-300 ease-in-out">
                  <li>Study Anon !!</li>
                </ul>
               </li>
          </ul>
        </div>
      </div>

      <div class="mt-3">
        <h1 class="text-4xl mt-[57px] mb-3 font-serif">
          Flash Attention from a Beginner's Point of View
        </h1>
        <span class="text-sm text-gray-500">23rd March, 2025</span>
        <p class="mt-4 font-serif text-lg">
          Transformers are amazing, right? Their attention mechanisms like self-attention and multi-head attention make them super powerful for understanding context in text, translations, and more. But there’s a catch: traditional attention can be a memory hog and slowpoke, especially when dealing with long sequences. Enter <span class="font-bold font-serif">Flash Attention</span>, a clever optimization that makes attention faster and more efficient without sacrificing accuracy. Let’s dive in!
        </p>

        <p class="my-4 text-lg font-serif">This is just merely part I of what Flash attention is in upcoming parts of Flash attention we will be diving deeper into flash attention internals and implementation in pytorch</p>
      
        <h1 class="mt-8 font-serif text-3xl">What is Flash Attention and Why is it Needed?</h1>
        <p class="mt-4 font-serif text-lg">
          Attention, as we know, computes a weighted sum of values (V) based on how relevant each key (K) is to a query (Q). The problem? For a sequence of length N, the attention mechanism needs to create an N × N attention score matrix. That’s a lot of memory! For example, if N = 10,000 (think a long document), you’re storing 100 million numbers just for that matrix. GPUs, which power these models, choke on this because they have limited memory, and moving data back and forth slows everything down.
        </p>
        <p class="mt-4 font-serif text-lg">
          Flash Attention is a way to compute attention without ever fully building that giant N × N matrix in memory. Instead, it processes the data in smaller chunks (or "tiles") and does the math on-the-fly. It’s like solving a puzzle piece by piece instead of laying out the whole picture at once. This saves memory, speeds things up, and lets Transformers handle much longer sequences—like entire books—without crashing.
        </p>
      
        <h1 class="text-2xl underline underline-offset-4 font-serif mt-6">Why is it Needed?</h1>
        <ul class="list-disc mt-4 ml-5">
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Memory Efficiency:</span> Traditional attention’s memory usage grows quadratically (N²), while Flash Attention scales linearly (N).</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Speed:</span> It reduces redundant data movement between GPU memory layers.</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Scalability:</span> It unlocks Transformers for super-long sequences (e.g., 64k tokens instead of 512).</p>
          </li>
        </ul>
      
        <h1 class="mt-8 font-serif text-3xl">How Does Flash Attention Work?</h1>

        <img src="../blogs/assets/flash-atten/flash-banner.png" class="rounded-xl my-5" alt="">
        <p class="mt-4 font-serif text-lg">
          Let’s break it down :
        </p>
        <ul class="list-disc mt-4 ml-5">
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">Compute the dot product of Q and K to get the attention scores (N × N matrix).</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">Scale and apply softmax to turn scores into weights.</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">Multiply those weights by V to get the output.</p>
          </li>
        </ul>
        <p class="mt-4 font-serif text-lg">
          Flash Attention says, “Why store that huge score matrix?” Instead, it:
        </p>
        <ul class="list-disc mt-4 ml-5">
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">Splits Q, K, and V into smaller blocks (like cutting a big cake into slices).</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">Computes attention for one block at a time, using a technique called tiling.</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">Updates the output incrementally, keeping only what’s needed in memory.</p>
          </li>
        </ul>
        <p class="mt-4 font-serif text-lg">
          Here’s the trick: it avoids storing the full attention score matrix by recomputing intermediate results when necessary and using clever math to keep the softmax accurate across blocks. This happens entirely on the GPU’s fast memory (SRAM), skipping the slower main memory (HBM).
        </p>
      
        <h1 class="text-2xl underline underline-offset-4 font-serif mt-6">Step-by-Step:</h1>
        <ol class="list-decimal mt-4 ml-5">
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Input:</span> Q, K, and V matrices (say, each is N × d, where d is the embedding size).</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Tiling:</span> Break them into smaller chunks (e.g., blocks of size B × d, where B << N).</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Inner Loop:</span> For each block of Q, compute attention with all blocks of K and V, but only store tiny temporary results.</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Softmax Trick:</span> Use a running normalization to combine results across blocks without ever needing the full matrix.</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Output:</span> Build the final attention output block-by-block.</p>
          </li>
        </ol>
        <p class="mt-4 font-serif text-lg">
          The result? Same output as regular attention, but way less memory and faster computation.
        </p>
      
        <h1 class="mt-8 font-serif text-3xl">Mathematical Representation</h1>
        <p class="mt-4 font-serif text-lg">
          Let’s keep it simple but precise. Regular attention computes:
        </p>
        <p class="mt-4 font-serif text-lg text-center">
          <span class="italic">Attention(Q, K, V) = softmax(</span><span class="frac"><sup>QK<sup>T</sup></sup><sub>√d<sub>k</sub></sub></span><span class="italic">)V</span>
        </p>
        <p class="mt-4 font-serif text-lg">
          Where:
        </p>
        <ul class="list-disc mt-4 ml-5">
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">\( Q, K, V \in \mathbb{R}^{N \times d} \) (N = sequence length, d = dimension).</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">\( QK^T \in \mathbb{R}^{N \times N} \) (the big, problematic score matrix).</p>
          </li>
        </ul>
        <p class="mt-4 font-serif text-lg">
          Flash Attention avoids materializing \( QK^T \) fully. Instead:
        </p>
        <ul class="list-disc mt-4 ml-5">
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">Split \( Q \) into blocks \( Q_1, Q_2, ..., Q_m \) (each \( B \times d \)).</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">Split \( K \) and \( V \) similarly into \( K_1, K_2, ..., K_m \) and \( V_1, V_2, ..., V_m \).</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif">For each block \( Q_i \):</p>
            <ul class="list-disc ml-8">
              <li class="my-4 text-lg font-serif">Compute \( S_i = Q_i K^T \) (small \( B \times N \) matrix).</li>
              <li class="my-4 text-lg font-serif">Scale: \( S_i / \sqrt{d_k} \).</li>
              <li class="my-4 text-lg font-serif">Apply softmax incrementally, tracking running sums and normalization constants.</li>
              <li class="my-4 text-lg font-serif">Compute \( O_i = \text{softmax}(S_i)V \) and add to the output.</li>
            </ul>
          </li>
        </ul>
        <p class="mt-4 font-serif text-lg">
          The magic is in the incremental softmax, which uses two extra variables (a max and a sum) to stitch everything together accurately without the full matrix.
        </p>
      
        <p class="mt-4 font-serif text-lg">
          With Flash Attention, the operator is smarter. They only look at one section of the stage at a time (a block), figure out who’s important in that moment, and adjust the spotlight right away. They keep a tiny notepad (fast GPU memory) with just enough info to move to the next section. The play (computation) goes on smoothly, and the audience (model) still sees the full story—no one notices the operator’s trick!
        </p>
      
        <h1 class="mt-8 font-serif text-3xl">Flash Attention in the Transformer Architecture</h1>
        <p class="mt-4 font-serif text-lg">
          Flash Attention fits right into the Transformer’s attention layers:
        </p>
        <ul class="list-disc mt-4 ml-5">
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Encoder:</span> Replaces self-attention with Flash Attention to process input sequences efficiently.</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Decoder:</span> Handles masked self-attention (for previous tokens only) and cross-attention (connecting to the encoder) with the same block-wise trick.</p>
          </li>
        </ul>
        <p class="mt-4 font-serif text-lg">
          It’s plug-and-play: same Transformer, just faster and leaner.
        </p>
      
        <h1 class="mt-8 font-serif text-3xl">Why Is Flash Attention a Big Deal?</h1>
        <ul class="list-disc mt-4 ml-5">
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Longer Sequences:</span> Models can now process tens of thousands of tokens (e.g., 64k) instead of hitting a wall at 512 or 1024.</p>
          </li>
          <li class="my-4">
            <p class="mt-4 text-lg font-serif"><span class="font-bold">Energy Savings:</span> Less memory movement = less power, which matters for big AI training runs.</p>
          </li>
        </ul>

        <h1 class="mt-8 font-serif text-3xl">Intuition: Flash Attention as a Spotlight</h1>
        <p class="mt-4 font-serif text-lg">
          Remember our theater analogy? In regular attention, the spotlight operator (Attention) scans the entire stage (all tokens) at once, writing down a huge list of who’s important (the N × N matrix) before deciding where to shine the light. With a big stage, that list gets unwieldy, and the operator runs out of paper (memory).
            <br>
          With Flash Attention, the operator is smarter. They only look at one section of the stage at a time (a block), figure out who’s important in that moment, and adjust the spotlight right away. They keep a tiny notepad (fast GPU memory) with just enough info to move to the next section. The play (computation) goes on smoothly, and the audience (model) still sees the full story—no one notices the operator’s trick!
        </p>
      </div>


      <hr class="my-10" />
    <footer class="my-8">By Mrinal</footer>

    <script>
      function toggleCollapse(id) {
        const element = document.getElementById(id);
        if (element.classList.contains('hidden')) {
          element.classList.remove('hidden');
        } else {
          element.classList.add('hidden');
        }
      }
    </script>
    
</body>
</html>